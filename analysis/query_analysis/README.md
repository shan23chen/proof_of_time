# Agent Query Analysis

Analyze agent tool calls, token usage, cost, and correlations across evaluation runs.

## Directory Structure

```
query_analysis/
├── analyze_agent_queries.py       # Main analysis script
├── correlation_analysis.py        # Correlation and aggregate stats
├── inputs/                         # (Reserved for future inputs)
├── outputs/                        # Generated analysis outputs
│   ├── agent_query_analysis_msg_limits.csv  # Per-run query statistics
│   ├── correlation_matrix.csv               # Correlation coefficients
│   └── AGENT_QUERY_ANALYSIS.md              # Detailed analysis report
└── README.md                       # This file
```

## Usage

### Analyze Agent Queries

Extract tool calls, token usage, and cost from evaluation logs:

```bash
# Analyze all runs (uses default summary CSVs)
python analysis/query_analysis/analyze_agent_queries.py

# Specify custom summary files
python analysis/query_analysis/analyze_agent_queries.py \
  --summary-files logs_msg15_summary.csv logs_msg30_summary.csv logs_msg50_summary.csv

# Custom output path
python analysis/query_analysis/analyze_agent_queries.py \
  --output analysis/query_analysis/outputs/custom_results.csv
```

### Correlation Analysis

Compute correlations between queries, tokens, cost, and performance:

```bash
# Analyze correlations (uses default input)
python analysis/query_analysis/correlation_analysis.py

# Custom input file
python analysis/query_analysis/correlation_analysis.py \
  --input analysis/query_analysis/outputs/agent_query_analysis_msg_limits.csv
```

## Inputs

### Required
Summary CSV files in project root (generated by main analysis pipeline):
- `logs_msg15_summary.csv`
- `logs_msg30_summary.csv`
- `logs_msg50_summary.csv`

These contain metadata linking evaluation runs to message limits and accuracy scores.

### Evaluation Logs
The scripts read Inspect AI `.eval` files (ZIP archives) referenced in the summary CSVs.

## Outputs

### Main Analysis CSV
`outputs/agent_query_analysis_msg_limits.csv`

Per-run statistics including:
- **Tool calls**: Total and per-sample counts
- **Token usage**: Input, output, cache read/write tokens
- **Cost**: Estimated USD cost using model pricing
- **Duration**: Wall-clock execution time
- **Accuracy**: Task performance score
- **Message limit**: Agent budget (15/30/50 turns)

### Correlation Matrix
`outputs/correlation_matrix.csv`

Pearson correlation coefficients between all metrics (successful runs only).

### Analysis Report
`outputs/AGENT_QUERY_ANALYSIS.md`

Detailed findings including:
- Summary statistics by task, model, message limit
- Top correlated metrics
- Cost breakdown
- Performance bottlenecks

## Key Metrics

| Metric | Description |
|--------|-------------|
| `total_tool_calls` | Number of agent tool invocations |
| `avg_tool_calls_per_sample` | Tool calls normalized by task count |
| `input_tokens` | Tokens sent to model |
| `output_tokens` | Tokens generated by model |
| `cache_read_tokens` | Prompt cache hits |
| `cache_write_tokens` | Prompt cache writes |
| `estimated_cost_usd` | Estimated cost using model pricing |
| `accuracy` | Task completion accuracy |

## Model Pricing

The script includes pricing for:
- **OpenAI**: GPT-5.x series (including cache pricing)
- **Anthropic**: Claude 4.5 Haiku/Sonnet/Opus
- **Google**: Gemini 2.5/3.x series
- **Groq**: Llama, Kimi, Qwen (free tier)

Prices are approximate and per million tokens. See `MODEL_PRICING` in `analyze_agent_queries.py` for details.

## Example Workflow

```bash
# 1. Generate query analysis
python analysis/query_analysis/analyze_agent_queries.py

# 2. Compute correlations and summary stats
python analysis/query_analysis/correlation_analysis.py

# 3. Review outputs
cat analysis/query_analysis/outputs/AGENT_QUERY_ANALYSIS.md
```

## Notes

- Analysis focuses on successful runs (`status == 'success'`)
- Failed/cancelled runs are excluded from correlations
- Token counts include prompt caching for accurate cost estimates
- Message limit information extracted from eval log paths
- Accuracy values preserved from existing CSVs when re-running analysis
